{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise 9.1 Policy Gradient on Continuous CartPole\n",
    "\n",
    "## Goal\n",
    "\n",
    "- understanding policy gradient and implement it\n",
    "- understand how each hyperparameter contributes to the learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import chula_rl as rl\n",
    "from chula_rl.env.cartpolecont import ContinuousCartPoleEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    env = ContinuousCartPoleEnv()\n",
    "    env = rl.env.wrapper.EpisodeSummary(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Parallel Env (VecEnv)\n",
    "\n",
    "This kind of env will take a vector of actions, returns a vector of states. This will help stabilize training (and also speed up) greatly especially in on-policy learning.\n",
    "\n",
    "Example of 2 parallel envs (you could use any):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = rl.env.DummyVecEnv([make_env] * 2)\n",
    "s = env.reset()\n",
    "print('s.shape:', s.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see (2, 4) which means 2 envs of 4 features (normal to CartPole).\n",
    "\n",
    "An interesting part of the parallel env is that it will \"reset\" the underlying env automatically (when it is done). This means we can always take action, do not need to care of the underlying environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Continuous CartPole\n",
    "\n",
    "This is the same as a normal CartPole. The only difference is that the action space is \"continuous\" dictated by a single \"float\" within (-1, 1). \n",
    "\n",
    "Exmaple of taking action in a parallel env: \n",
    "\n",
    "Each action has 1 dimension, parallel action becomes 2 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss, r, done, info = env.step(np.array([[-0.8], [1.0]]))\n",
    "print('ss.shape:', ss.shape)\n",
    "print('r.shape:', r.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Vec n-step Explorer\n",
    "\n",
    "In a parallel environment setting, we also need a compatible parallel explorer. The code is straightforward to the point that we have implemented it for you already. But you are welcome to read the code. \n",
    "\n",
    "Go see `chula_rl.explorer.vec_many_step_explorer`\n",
    "\n",
    "In policy gradient, we usually use an n-step return of some kind because it is more stable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = rl.explorer.VecManyStepExplorer(n_step, n_max_interaction, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Advantage Actor-Critic (A2C) policy + n-step TD residual advantage\n",
    "\n",
    "A2C requires two components: \n",
    "- Actor (policy)\n",
    "- Critic (value function) \n",
    "\n",
    "Both are implemented as neural nets. We leave this section to you. \n",
    "\n",
    "Your A2C should subclass `chula_rl.policy.base_policy.BasePolicy`. \n",
    "\n",
    "## Words of advice: \n",
    "\n",
    "- You code will surely contain bugs! Developing in jupyter notebook might not be a good idea. \n",
    "- There is a ton of hyperparameters, it is no easy task to find the right parameters\n",
    "- Finding the right parameters might need some analysis on how the code performs which is hard if you don't \"log\" enough\n",
    "- So, log EVERYTHING, use tensorboard to your advantage\n",
    "- For example, log the std of the policy, log the current value of the value function. These will be invaluable in debugging\n",
    "- \"‡∏ó‡∏≥‡πÑ‡∏°‡∏°‡∏±‡∏ô‡∏ä‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏£‡∏≤‡∏∞‡∏ö‡∏≤‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÄ‡∏Å‡∏¥‡∏ô ~\" is a sentence to describe this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run it\n",
    "\n",
    "If you forgot how to run it already. Here is how: \n",
    "\n",
    "```\n",
    "while True:\n",
    "    data = exp.step(policy)\n",
    "    policy.optimize_step(data)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: A2C + n-step Generalized Advantage\n",
    "\n",
    "You are invited to implement the same A2C but using the generalized advantage instead. Legend has it this is a better advantage estimate! üòé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:charin]",
   "language": "python",
   "name": "conda-env-charin-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
