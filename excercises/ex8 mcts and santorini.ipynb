{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise 8 Model-based Monte Carlo tree search on Santorini environment\n",
    "\n",
    "## Goals:\n",
    "- able to adapt an environment to work with MCTS\n",
    "- implement MCTS\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- pull updates of https://github.com/phizaz/chula_rl \n",
    "- you will need to write code in python (not just in the notebooks)\n",
    "- a good code editor is preferable\n",
    "\n",
    "## Things you need to implement:\n",
    "\n",
    "1. **MCTS** `chula_rl.alphazero.mcts`, Monte-Carlo tree search file. We have coded the template, but you need to fill in the blanks.\n",
    "2. **Game interface** `chula_rl.alphazero.santorini.game`, You need to complete the interface between an environment and MCTS. We have coded the template, you need to fill the blanks.\n",
    "3. **Network** `chula_rl.alphazero.santorini.net.tf`, the main neural network shoud be implemented in TF2. \n",
    "\n",
    "## Hints\n",
    "\n",
    "`chula_rl.alphazero.othello` is an example of adapting an environment (the game of Othello) to work with MCTS interface. You can take it as an inspiration for the Santorini. \n",
    "\n",
    "However, you still need to implement the MCTS code which is not provided. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disclaimer: most of the code in the repo is not developed by us. It is from an annonymous origin for now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 Understanding the game of Santorini\n",
    "\n",
    "Youtube explanation from the author of the game: https://www.youtube.com/watch?v=EzHAykZTCHU\n",
    "\n",
    "We don't really play the \"original\" Santorini, we play a simpler one in which there are no \"god powers\" cards.\n",
    "\n",
    "Python implementation of the game is available at: https://github.com/cstorm125/santorini (hats off to Charin üôè)\n",
    "\n",
    "We have made some changes to it ... so use the one in `chula_rl.alphazero.santorini.santorinigo.environment` instead. \n",
    "\n",
    "Details are seen in the repo under sections \"Setup\" and \"Each turn\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 implement the game interface \n",
    "\n",
    "You might want to play around to make sure that it does what it should do. Checking each method one by one is a good idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chula_rl.alphazero.santorini.game import SantoriniGame\n",
    "\n",
    "game = SantoriniGame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 implement the neural network\n",
    "\n",
    "Neural network in this case hase \"two\" heads.\n",
    "\n",
    "One for action distribution. One for state-value prediction.\n",
    "\n",
    "Make sure that the output shape makes sense.\n",
    "\n",
    "**Feel free to change the existing code.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chula_rl.alphazero.santorini.net.tf import Backbone, SantoriniNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 implement MCTS\n",
    "\n",
    "You need to implement the file `chula_rl.alphazero.mcts`. We have already provided some structure of it. The code should be fairly close to the pseucode in the classroom. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chula_rl.alphazero.mcts import MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 Train an agent with MCTS\n",
    "\n",
    "To train an agent under the MCTS framework there are a few ingredients to consider (all of which are provided). \n",
    "\n",
    "We have provided:\n",
    "\n",
    "- **Coach** `chula_rl.alphazero.coach` this is the main \"traning loop\". Basically, you call `coach.learn`\n",
    "- **Arena** which is used by **Coach** for competing two policies to see which one fares better. **Coach** uses this to determine if the improved policy is really better, if not it is discarded.\n",
    "\n",
    "Coach will automatically save neural nets for each step of progresses. You can control where to save in `Args` (default `./checkpoint/`)\n",
    "\n",
    "### Advice\n",
    "\n",
    "Training takes a very longggg time. If you want to debug, set args so that it would run fast to make sure your code work. Only when you are pretty sure about your code, run with a larger setting! üëå"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chula_rl.alphazero.coach import *\n",
    "\n",
    "# config for the coach\n",
    "args = Args() \n",
    "\n",
    "def make_net():\n",
    "    # implement this \n",
    "    # ... it should return the neural network\n",
    "    # return SantoriniNet()\n",
    "    raise NotImplementedError()\n",
    "\n",
    "g = SantoriniGame()\n",
    "\n",
    "# logging is very useful to see if we make any progress!\n",
    "writer = tf.summary.create_file_writer(f'tensorboard')\n",
    "with writer.as_default():\n",
    "    c = Coach(g, make_net, args)\n",
    "    c.learn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 comparing agents\n",
    "\n",
    "You could use **Arena** to compare many kinds of agents. You could compete the best agent in the `./checkpoint/` to compare with other versions of itself. Or with a random agent. \n",
    "\n",
    "Note: a baseline from us might be released later on.\n",
    "\n",
    "### Provided agents in `chula_rl.alphazero.santorini.players`\n",
    "\n",
    "- Human: you can control an agent by yourself, albeit you need to know the exact command for it (which means you need to dig deeper into the codebase üòâ)\n",
    "- Random: a random player. Your policy should be better than random agent at the very least. Show us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chula_rl.alphazero.arena import Arena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we grade?\n",
    "\n",
    "We look for the effort you put into making MCTS work with Santorini. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:charin]",
   "language": "python",
   "name": "conda-env-charin-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
